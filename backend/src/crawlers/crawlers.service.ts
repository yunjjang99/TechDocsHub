//     // í¬ë¡¤ë§ ë¡œì§ êµ¬í˜„ ì˜ˆì •
//     //1.ìŠ¤ì¼€ì¥´ë§ëœ ìŠ¤í¬ë˜í•‘ ë¡œì§ì„ Cronìœ¼ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë¨ ì£¼ê¸°ëŠ” 1ì¼ì— 1ë²ˆì´ë˜ë„ë¡
//     //2.ì´ì „ì— ê°€ì ¸ì˜¨ ëª©ë¡ê³¼ í˜„ì¬ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ HTMLì„ ê°€ì ¸ì˜¨ hrefë¥¼ ë¹„êµí•˜ì—¬ ë¸”ë¡œê·¸ê¸€ë“¤ì„ ë¹„êµí•¨
//     //3.í˜„ì¬ ì¶”ê°€ë˜ì–´ìˆëŠ” ë§í¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ê¸€ì˜ HTML ì„ Parse ì´ë¯¸ì§€ëŠ” ë¡œì»¬ì— ì €ì¥(ê¸€ì„ ì•„ì¹´ì´ë¹™í•˜ê¸°ì—ëŠ” MDì™€ HTMLê³¼ ì´ë¯¸ì§€íŒŒì¼ì„ ë¡œì»¬ì— ì €ì¥ í˜¹ì€ S3ì— ì €ì¥)
//     //4.parseëœ ê¸€ì„ mdë¡œ ë°˜í™˜ í›„ ë²ˆì—­ (ì •ë¦¬ ë° ë²ˆì—­ë¶€ë¶„ì€ AIë¥¼ ì´ìš©?)
//     //ìŠ¤ì¼€ì¥´ë§ì€ 1ë¶„ì— 1ê°œì˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•¨ì„ ë³´ì¥ -> ì´ ê²½ìš°ì—ëŠ” 60ê°œì˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ 1ì‹œê°„ì— ì²˜ë¦¬ ê·¸ëŸ¬ë©´ ìƒˆë²½ 2ì‹œì— Cronì„ ëŒë¦¬ëŠ”ê²ƒìœ¼ë¡œí•˜ì
//     //ìŠ¤í¬ë˜í•‘ ë¡œì§ì€ ê°€ì¥ìµœê·¼ì˜ ë¸”ë¡œê·¸ê¸€ë§Œ ê°€ì ¸ì˜¤ë©´ë˜ê¸°ë•Œë¬¸ì— (í•˜ë£¨ì— 2ê°œì´ìƒ ì˜¬ë¼ì˜¤ì§€ì•ŠëŠ”ë‹¤ëŠ” ê¸°ì¤€ìœ¼ë¡œ) / ë§Œì•½ í•˜ë£¨ì— 2ê°œì´ìƒì˜¬ë¼ì˜¨ë‹¤ë©´?  ìŠ¤í¬ë¡¤ì´ë‚˜ ì²«BASE URLë¡œì ‘ê·¼í•œ  HTMLì—
//     //ìµœì‹ ì˜ ëª¨ë“  ë¸”ë¡œê·¸ê¸€ì´ ì—†ë‹¤ë©´? ì ì‘í˜• ìŠ¤í¬ë˜í•‘ì´ í•„ìš”í•¨
//     //

import { Injectable, OnModuleInit } from "@nestjs/common";
import { InjectRepository } from "@nestjs/typeorm";
import { Repository } from "typeorm";
import { Crawler } from "./entities/crawlers.entity";
import { BlogPost } from "@/types/interfaces";
import NetflixBlogCrawler from "./domain/netflix/netflix-blog-crawler";
import { Cron } from "@nestjs/schedule";

@Injectable()
export class CrawlersService implements OnModuleInit {
  constructor(
    @InjectRepository(Crawler)
    private readonly crawlerRepository: Repository<Crawler>,
    private readonly netflixBlogCrawler: NetflixBlogCrawler
  ) {}

  async onModuleInit() {
    console.log("ğŸš€ Application started - Executing initial crawl...");
    await this.netflixBlogCrawler.netflixCrawl();
  }

  @Cron("0 1 * * *") //1AM ìŠ¤í¬ë˜í•‘
  handleCron() {
    setTimeout(async () => {
      await this.netflixBlogCrawler.netflixCrawl();
    }, 1000);
  }

  async saveCrawlerData(blogPost: BlogPost): Promise<void> {
    const crawler = new Crawler();
    crawler.companyName = blogPost.companyName;
    crawler.baseBlogUrl = blogPost.baseBlogUrl;
    crawler.blogPostUrl = blogPost.blogPostUrl;
    crawler.title = blogPost.title;
    crawler.postPublishedAt = blogPost.postPublishedAt;
    crawler.crawledAt = blogPost.crawledAt;
    crawler.absolutePath = blogPost.absolutePath;
    crawler.relativePath = blogPost.relativePath;
    crawler.status = blogPost.status;
    crawler.description = blogPost.description;
    crawler.tags = blogPost.tags;
    crawler.retryCount = blogPost.retryCount;
    crawler.language = blogPost.language;

    await this.crawlerRepository.save(crawler);
    console.log(`ë¸”ë¡œê·¸ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ${blogPost.title}`);
  }

  async getScrapedUrls(companyName): Promise<string[]> {
    const crawledData = await this.crawlerRepository.find({
      where: { companyName },
      select: ["blogPostUrl"],
    });
    return crawledData.map((data) => data.blogPostUrl);
  }
}
